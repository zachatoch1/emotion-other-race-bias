{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import base64\n",
    "import time\n",
    "import sys\n",
    "# load open-api key\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# define utility functions\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "# define chat completion function\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# define chat completion function\n",
    "def complete_chat(prompt, base64_image):\n",
    "    global client\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-vision-preview\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,\n",
    "                        },\n",
    "                        {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"low\"\n",
    "                        }\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=1,\n",
    "        )\n",
    "        response_content = response.choices[0].message.content\n",
    "        return (int(response_content), None) if response_content.isdigit() and 1 <= int(response_content) <= 7 else (None, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(sys.exc_info())\n",
    "        return True, e\n",
    "    \n",
    "# Define a function to process a single image\n",
    "def process_image(text_prompt, base64_image, image_race, asking_emotion, image_emotion, rater_race):\n",
    "    response, e = complete_chat(text_prompt, base64_image)\n",
    "    return {\"image_race\": image_race, \"asking_emotion\": asking_emotion, \"image_emotion\": image_emotion,\"rater_race\": rater_race, \"response\": response}, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template and api completion parameters\n",
    "prompt=\"\"\"Imagine you are a{race} person. Your life experiences and background have been largely shaped by your being{race}. As a{race} person you feel your perception of yourself and others is unique. Today you have signed up for a survey to help researchers understand how people perceive emotions in images. \n",
    "\n",
    "Researcher: Please look at the image provided. We are interested in understanding how well this image represents the emotion of {emotion}. Using the scale provided below, please indicate your level of agreement with the following statement:\n",
    "\"This image accurately represents the emotion of {emotion}.\"\n",
    "\n",
    "1. Strongly Disagree\n",
    "2. Disagree\n",
    "3. Somewhat Disagree\n",
    "4. Neutral\n",
    "5. Somewhat Agree\n",
    "6. Agree\n",
    "7. Strongly Agree\n",
    "\n",
    "Please select the number that best reflects your perception of the image. Respond with the number rating only and nothing else.\n",
    "\n",
    "{race} Person:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "try:\n",
    "        df = pd.read_csv(\"rawdata/results.csv\").fillna(\"\")\n",
    "        results = df.to_dict(orient=\"records\")\n",
    "        done = df.groupby([\"image_race\", \"asking_emotion\", \"rater_race\", \"image_emotion\"]).count().reset_index()\n",
    "except:\n",
    "       done = pd.DataFrame(columns=[\"image_race\", \"asking_emotion\", \"rater_race\", \"image_emotion\", \"response\"])\n",
    "       pass\n",
    "\n",
    "# define list of rater races\n",
    "rater_races = [\" white\", \" black\", \"\"]\n",
    "\n",
    "# define rate limit tracking variables\n",
    "tokens = 0\n",
    "time_start = time.time()\n",
    "# define list of images\n",
    "images = os.listdir(\"images\")\n",
    "\n",
    "for image in images:\n",
    "    for rater_race in rater_races:\n",
    "        # getting data necessary for gathering\n",
    "        image_emotion, asking_emotion, image_race = image.split(\".\")[0].split(\"_\")\n",
    "        image_path = f\"images/{image}\"\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "        text_prompt = prompt.format(race=rater_race, emotion=asking_emotion)\n",
    "        # checking if image has been rated\n",
    "        curdone = done[(done[\"image_race\"] == image_race) & (done[\"asking_emotion\"] == asking_emotion) & (done[\"rater_race\"] == rater_race) & (done[\"image_emotion\"] == image_emotion)].reset_index(drop=True)\n",
    "        if len(curdone) > 0:\n",
    "                curdone = curdone.loc[0, \"response\"]\n",
    "                num_trials = 100 - curdone\n",
    "        else:\n",
    "                num_trials = 100\n",
    "        print(\"\\n\\n\")\n",
    "        print(f\"Num Trials: {num_trials} left for asking emotion {asking_emotion}, image emotion {image_emotion}, image race {image_race}, and Rater Race {rater_race}. Image path is {image_path}\")\n",
    "        print(\"\\n\\n\")\n",
    "        #print(text_prompt)\n",
    "        # running trials\n",
    "        for trial_num in range(num_trials):\n",
    "            # making sure responses are valid\n",
    "                valid = False\n",
    "                while not valid:\n",
    "                        # check rate limit\n",
    "                        if tokens + 289 > 10000:\n",
    "                                time_end = time.time()\n",
    "                                time_elapsed = time_end - time_start\n",
    "                                if time_elapsed < 60:\n",
    "                                        time.sleep(60 - time_elapsed)\n",
    "                                tokens = 0\n",
    "                                time_start = time.time()\n",
    "                        \n",
    "                        # fetch response\n",
    "                        response, e = process_image(text_prompt, base64_image, image_race, asking_emotion, image_emotion, rater_race)\n",
    "                        tokens += 289\n",
    "                        print(response)\n",
    "                        # validating response\n",
    "                        if (response[\"response\"] is not None and response[\"response\"] != True) or response[\"response\"] == 1:\n",
    "                                valid = True\n",
    "                                results.append(response)\n",
    "                                tokens += 289\n",
    "                        elif response[\"response\"] == True:\n",
    "                                print(f\"Error: {e}\")\n",
    "                                sys.exit(5)\n",
    "                \n",
    "                # checkpoint df\n",
    "                df = pd.DataFrame(results)\n",
    "                df.to_csv(\"rawdata/ckpt.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv with double check for fallback\n",
    "df = pd.read_csv(\"rawdata/ckpt.csv\").fillna(\"\")\n",
    "try:\n",
    "        old_df = pd.read_csv(\"rawdata/results.csv\").fillna(\"\")\n",
    "except:\n",
    "        old_df = pd.DataFrame(columns=[\"image_race\", \"asking_emotion\", \"image_emotion\", \"rater_race\", \"response\"])\n",
    "print(\"New DF\")\n",
    "display(df.groupby([\"image_race\", \"asking_emotion\", \"image_emotion\", \"rater_race\"]).count().reset_index())\n",
    "print(\"Old DF\")\n",
    "display(old_df.groupby([\"image_race\", \"asking_emotion\", \"image_emotion\", \"rater_race\"]).count().reset_index())\n",
    "if input(\"Would you like to overwrite: \") == \"y\":\n",
    "        df.to_csv(\"rawdata/results.csv\", index=False)\n",
    "        print(\"Overwritten\")\n",
    "        os.remove(\"rawdata/ckpt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformatting data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rawdata/results.csv\").fillna(\"\")\n",
    "analysis_data = pd.DataFrame()\n",
    "# converting races to dummies \n",
    "analysis_data[\"is_image_white\"] = df[\"image_race\"].apply(lambda x: int(x == \"W\"))\n",
    "analysis_data[\"is_image_black\"] = df[\"image_race\"].apply(lambda x: int(x == \"B\"))\n",
    "analysis_data[\"is_rater_white\"] = df[\"rater_race\"].apply(lambda x: int(x == \" white\"))\n",
    "analysis_data[\"is_rater_black\"] = df[\"rater_race\"].apply(lambda x: int(x == \" black\"))\n",
    "analysis_data[\"is_rater_default\"] = df[\"rater_race\"].apply(lambda x: int(x == \"\"))\n",
    "analysis_data[\"other_race\"] = analysis_data.apply(lambda x: 1 if x[\"is_image_white\"] != x[\"is_rater_white\"] and x[\"is_rater_default\"] == 0 else 0, axis=1)\n",
    "analysis_data[\"emotion_congruence\"] = df[\"asking_emotion\"] == df[\"image_emotion\"]\n",
    "analysis_data[\"emotion_congruence\"] = analysis_data[\"emotion_congruence\"].apply(lambda x: int(x))\n",
    "\n",
    "# adding response\n",
    "analysis_data[\"rating\"] = df[\"response\"]\n",
    "\n",
    "# adding emotion for potential categorical analysis\n",
    "analysis_data[\"image_emotion\"] = df[\"image_emotion\"]\n",
    "\n",
    "# define variable labels\n",
    "variable_labels = {\n",
    "    \"is_image_white\": \"Indicator if image is of a white person\",\n",
    "    \"is_image_black\": \"Indicator if image is of a black person\",\n",
    "    \"is_rater_white\": \"Indicator if rater is white\",\n",
    "    \"is_rater_black\": \"Indicator if rater is black\",\n",
    "    \"is_rater_default\": \"Indicator if rater race is not specified\",\n",
    "    \"other_race\": \"Indicator if image and rater races differ, 0 if rater is default\",\n",
    "    \"emotion_congruence\": \"Indicator of same emotion between asking and image, 1 if same\",\n",
    "    \"rating\": \"Rater's response\",\n",
    "    \"image_emotion\": \"Emotion displayed in the image\"\n",
    "}\n",
    "\n",
    "# save data for stata\n",
    "analysis_data.to_stata(\"rawdata/analysis_data.dta\", variable_labels=variable_labels, write_index=False)\n",
    "\n",
    "# display data for curiousity\n",
    "display(analysis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
